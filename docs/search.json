[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "This projects builds a Recurrent Neural Network model for the Natural Language Processing with Disaster Tweets competition, hosted on Kaggle (Howard et al. 2019), with the objective of developing a machine learning model that can accurately classify tweets as disaster-related or not. The dataset consists of 10,000 manually labeled tweets, creating a binary classification task where tweets are labeled 1 for disaster-related content and 0 non disaster-related content.\nTo achieve this goal, this project will develop a neural network model using PyTorch to build of a Recurrent Neural Network (RNN) model, which is should be well-suited for processing sequential text data. The final predictions generated by the trained model will be submitted to Kaggle for evaluation.\nTo achieve this goal, this project will leverage PyTorch (Ansel et al. 2024) to design and implement a Recurrent Neural Network (RNN), a neural architecture well-suited for processing sequential text data. The trained model will generate predictions that will be submitted to Kaggle for evaluation.\nThis project will address the following research questions:\n\n\n\nTable 1: Project Research Questions\n\n\n\n\n\n\n\n\n\nResearch Area\nQuestion\n\n\n\n\nData Preparation\nHow should text data be preprocessed to maximize model performance?\n\n\nModel Building\nHow do we implementation a RNN model in PyTorch?\n\n\nHyperparameter Tuning\nWhat static hyperparameters should be defined and what dynamic hyperparameters should be tuned?\n\n\nModel Performance\nWhat performance metrics should be used?How do the models perform during training, validation, and testing?\n\n\nImprovement Strategies\nWhat methods can be used to further enhance model performance?\n\n\n\n\n\n\nBeyond answering these questions, the project aims to address the technical challenges related to RNN models, including mitigating overfitting, handling exploding gradients, and balancing model complexity with prediction accuracy.\nThe workflow for this research is summarized in Figure 1. The process begins with exploratory data analysis and preprocessing, followed by model training. Hyperparameter tuning is iteratively applied to refine model performance, culminating in the development of final models for Kaggle submission.\n\n\n\n\n\n\nflowchart LR\n    EDA[\"&lt;div style='line-height:1.0;'&gt;Exploratory&lt;br&gt;Data&lt;br&gt;Analysis&lt;/div&gt;\"]\n    --&gt; Clean[\"&lt;div style='line-height:1.0;'&gt;Clean&lt;br&gt;Original&lt;br&gt;Data&lt;/div&gt;\"]\n    --&gt; BuildModel[\"&lt;div style='line-height:1.0;'&gt;Build&lt;br&gt;RNN&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; Train[\"&lt;div style='line-height:1.0;'&gt;Train&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; Tune[\"&lt;div style='line-height:1.0;'&gt;Tune&lt;br&gt;Hyperparameters&lt;/div&gt;\"]\n    --&gt; OutputFinal[\"&lt;div style='line-height:1.0;'&gt;Final&lt;br&gt;Models&lt;/div&gt;\"]\n    --&gt; Submit[\"&lt;div style='line-height:1.0;'&gt;Submit&lt;br&gt;Results&lt;/div&gt;\"]\n    Tune --&gt; Train\n\n\n\n\nFigure 1: RNN Project Workflow"
  },
  {
    "objectID": "index.html#training-data-columns-and-types",
    "href": "index.html#training-data-columns-and-types",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.1 Training Data Columns and Types",
    "text": "2.1 Training Data Columns and Types\n\nCode\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"../data/train.csv\")\ntrain_df.info()\n\n\n\n\nTable 2: Data Columns and Types of Training Data\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n\n\n\n\nTable 2 details the available data. Of note are large numbers of non existant data in the location column, and a small amount of data missing in the keyword column. id and target are numbers while the other 3 columns are text."
  },
  {
    "objectID": "index.html#training-data-sample",
    "href": "index.html#training-data-sample",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.2 Training Data Sample",
    "text": "2.2 Training Data Sample\n\n\nCode\ntrain_df.loc[train_df['location'].notna()].head()\n\n\n\n\nTable 3: Sample of Training Data\n\n\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n31\n48\nablaze\nBirmingham\n@bbcmtd Wholesale Markets ablaze http://t.co/l...\n1\n\n\n32\n49\nablaze\nEst. September 2012 - Bristol\nWe always try to bring the heavy. #metal #RT h...\n0\n\n\n33\n50\nablaze\nAFRICA\n#AFRICANBAZE: Breaking news:Nigeria flag set a...\n1\n\n\n34\n52\nablaze\nPhiladelphia, PA\nCrying out for more! Set me ablaze\n0\n\n\n35\n53\nablaze\nLondon, UK\nOn plus side LOOK AT THE SKY LAST NIGHT IT WAS...\n0\n\n\n\n\n\n\n\n\n\n\nTable 3 outputs the contents of a subset of the data. Data in the keyword column appears to be somewhat standardized while data in the location and text columns appear to be original inputs from the user."
  },
  {
    "objectID": "index.html#distribution-of-target-values",
    "href": "index.html#distribution-of-target-values",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.3 Distribution of Target Values",
    "text": "2.3 Distribution of Target Values\nIt is common for binary classification training data to have an equal weight of true and false values in the input. This is calculated by counting the occurrence of each value.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\n\n\nplt.figure(figsize=(7, 2.5))\nsns.countplot(x='target', data=train_df)\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Target Values\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Histogram of target Values in Training Data\n\n\n\n\n\nIn Figure 2 there are unequal values of each class in the data with a larger amount of negative values. This imbalance is important and must be accounted for during the model training and validation."
  },
  {
    "objectID": "index.html#sample-of-positive-tweets",
    "href": "index.html#sample-of-positive-tweets",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.4 Sample of Positive Tweets",
    "text": "2.4 Sample of Positive Tweets\n\n\nCode\ntrain_df.loc[train_df['target'] == 1].head()\n\n\n\n\nTable 4: Sample of Positive Training Data\n\n\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake M...\n1\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are ...\n1\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation or...\n1\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as ...\n1\n\n\n\n\n\n\n\n\n\n\nIn Table 4, positive tweets appear to have some relation to the disaster they are describing. The content appears to have multiple complex words and hashtags."
  },
  {
    "objectID": "index.html#sample-of-negative-tweets",
    "href": "index.html#sample-of-negative-tweets",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.5 Sample of Negative Tweets",
    "text": "2.5 Sample of Negative Tweets\n\n\nCode\ntrain_df.loc[train_df['target'] == 0].head()\n\n\n\n\nTable 5: Sample of Negative Training Data\n\n\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n15\n23\nNaN\nNaN\nWhat's up man?\n0\n\n\n16\n24\nNaN\nNaN\nI love fruits\n0\n\n\n17\n25\nNaN\nNaN\nSummer is lovely\n0\n\n\n18\n26\nNaN\nNaN\nMy car is so fast\n0\n\n\n19\n28\nNaN\nNaN\nWhat a goooooooaaaaaal!!!!!!\n0\n\n\n\n\n\n\n\n\n\n\nIn Table 5, negative tweets have content that appears irrelevant to a disaster. This content appears relatively generic and not specific to any event or location."
  },
  {
    "objectID": "index.html#word-count",
    "href": "index.html#word-count",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.6 Word Count",
    "text": "2.6 Word Count\nTo identify if data cleaning is necessary, the content of the tweets (text column) is visualized below. Each row is split into words lists by whitespace and a combined list of words is generated and counted.\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\ndef plot_word_horizontal_bar_chart(df, column, top_n=10, figsize=None):\n    \"\"\"\n    Plot a horizontal bar chart of the most common words.\n\n    :param words: List of words to analyze.\n    :param top_n: Number of most common words to display.\n    \"\"\"\n    # Count word frequencies\n    words = word_list = dataframe_to_word_list(train_df, column)\n    word_counts = Counter(words)\n    most_common = word_counts.most_common(top_n)\n\n    # Split words and their counts\n    labels, counts = zip(*most_common)\n\n    # print(labels[:10])\n    # print(counts[:10])\n\n    # # Plot the horizontal bar chart\n    if figsize is None:\n        figsize=(3, 6)\n    plt.figure(figsize=figsize)\n    plt.barh(labels, counts)\n    plt.xlabel(\"Count\")\n    # plt.ylabel(\"Words\")\n    plt.title(f\"Top {top_n} Words in {column}\")\n    plt.gca().invert_yaxis()  # Invert the y-axis to display the highest count at the top\n    plt.grid(axis='x', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\ndef dataframe_to_word_list(df, text_column):\n    \"\"\"\n    Convert a DataFrame column of text into a list of words.\n\n    :param df: The input DataFrame.\n    :param text_column: The name of the column containing text data.\n    :return: A list of words.\n    \"\"\"\n    # Tokenize each row into words and flatten into a single list\n    words = df[text_column].str.split().explode().tolist()\n    return [word for word in words if word and word is not np.nan]\n\n\n\n\nCode\nplot_word_horizontal_bar_chart(train_df, 'text', top_n=25)\nplot_word_horizontal_bar_chart(train_df.loc[train_df['location'].notna()], 'location', top_n=25)\nplot_word_horizontal_bar_chart(train_df, 'keyword', top_n=25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) text Column\n\n\n\n\n\n\n\n\n\n\n\n(b) location Column\n\n\n\n\n\n\n\n\n\n\n\n(c) keyword Column\n\n\n\n\n\n\n\nFigure 3: Original Data Word Count by Column\n\n\n\n\nThe word count histograms in Figure 3 reveals a mixture of relevant and possible unnecessary words and characters in each column. Additionally some characters may be removed to cleanup the input into the model."
  },
  {
    "objectID": "index.html#data-level-a1-removing-stop-words",
    "href": "index.html#data-level-a1-removing-stop-words",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.1 Data Level a1: Removing Stop Words",
    "text": "3.1 Data Level a1: Removing Stop Words\n\n\nCode\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\ndef filter_stop_words(df, column):\n    def filter_stop_words(word_list):\n        \"\"\"Filter stop words from a list of words.\"\"\"\n        words = str(word_list).split()\n        words = [word for word in words if word != \"nan\"]\n        return \" \".join([word for word in words if word.lower() not in stop_words and word])\n\n    # Apply the filtering function to the specified column\n    df[column] = df[column].apply(filter_stop_words)\n\n    return df\n\ntrain_df_a1 = train_df.copy()\n\ntrain_df_a1 = filter_stop_words(train_df_a1, 'text')\ntrain_df_a1 = filter_stop_words(train_df_a1, 'location')\ntrain_df_a1 = filter_stop_words(train_df_a1, 'keyword')\n\n\n\n\nCode\nplot_word_horizontal_bar_chart(train_df_a1, 'text', top_n=25)\nplot_word_horizontal_bar_chart(train_df_a1, 'location', top_n=25)\nplot_word_horizontal_bar_chart(train_df_a1, 'keyword', top_n=25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) text Column\n\n\n\n\n\n\n\n\n\n\n\n(b) location Column\n\n\n\n\n\n\n\n\n\n\n\n(c) keyword Column\n\n\n\n\n\n\n\nFigure 4: A1 Data Word Count by Column"
  },
  {
    "objectID": "index.html#data-level-a2-removing-unnecessary-characters",
    "href": "index.html#data-level-a2-removing-unnecessary-characters",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.2 Data Level a2: Removing Unnecessary Characters",
    "text": "3.2 Data Level a2: Removing Unnecessary Characters\nIt may be beneficial to further clean the data. We some high level techniques to normalize the input data.\n\n\nCode\nimport re\n\ndef clean_df_text_column(df, column):\n    def clean_row(tweet):\n        words = tweet.split()\n        cleaned_words = []\n        for word in words:\n            # Remove URLs\n            # word = re.sub(r'http\\S+|www\\S+', '[URL]', word)\n\n            # Replace user mentions (@username) with a placeholder\n            # word = re.sub(r'@\\w+', '[USER]', word)\n\n            # Remove hashtags but keep the word (e.g., \"#earthquake\" → \"earthquake\")\n            # word = re.sub(r'#(\\w+)', r'\\1', word)\n\n            # Remove unwanted characters (e.g., punctuation)\n            word = re.sub(r'[^\\w\\s]', '', word)\n\n            # Remove dashes\n            word = re.sub('-', '', word)\n\n            # Remove extra spaces (if any remain)\n            word = word.strip()\n\n            # Add the cleaned word to the list if it's not empty\n            if word and len(word) &gt; 1:\n                cleaned_words.append(word.lower())\n\n        return \" \".join(cleaned_words)\n\n    df[column] = df[column].apply(clean_row)\n\n    return df\n\ntrain_df_a2 = train_df_a1.copy()\n\ntrain_df_a2 = clean_df_text_column(train_df_a2, 'text')\ntrain_df_a2 = clean_df_text_column(train_df_a2, 'location')\ntrain_df_a2 = clean_df_text_column(train_df_a2, 'keyword')\n\n\n\n\nCode\nplot_word_horizontal_bar_chart(train_df_a2, 'text', top_n=25)\nplot_word_horizontal_bar_chart(train_df_a2, 'location', top_n=25)\nplot_word_horizontal_bar_chart(train_df_a2, 'keyword', top_n=25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) text Column\n\n\n\n\n\n\n\n\n\n\n\n(b) location Column\n\n\n\n\n\n\n\n\n\n\n\n(c) keyword Column\n\n\n\n\n\n\n\nFigure 5: A2 Data Word Count by Column"
  },
  {
    "objectID": "index.html#final-cleaning-results",
    "href": "index.html#final-cleaning-results",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.3 Final Cleaning Results",
    "text": "3.3 Final Cleaning Results\nFinal cleaning results from data level a2 are detailed below\n\n3.3.1 Text Content\nTo determine if there are differences between cleaned positive and negative tweets a sample of randomly selected tweets from each class is output below.\n\n3.3.1.1 Positive Tweets\n\n\nCode\n# train_df_a2[train_df_a2['target'] == 1].loc[train_df_a2['location'].str.len &gt; 1].head()\ntrain_df_a2[\n    (train_df_a2['target'] == 1) &\n    (train_df_a2['location'].str.len() &gt; 1) &\n    (train_df_a2['keyword'].str.len() &gt; 1)\n].sample(frac=1.0).head()\n\n\n\n\nTable 6: Sample of Cleaned Positive Training Data\n\n\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n4329\n6148\nhijack\nnigeria\ncriminals hijack lorries buses arrested enugu ...\n1\n\n\n5085\n7252\nnuclear20disaster\nnetherlands\nfukushimatepco fukushima nuclear disaster incr...\n1\n\n\n3871\n5503\nflames\nsanto domingo alma rosa\nsoloquiero maryland mansion fire killed caused...\n1\n\n\n6221\n8880\nsmoke\nktx\nget smoke shit peace\n1\n\n\n5710\n8147\nrescuers\niminchina\nvideo were picking bodies water rescuers searc...\n1\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.1.2 Negative Tweets\n\n\nCode\n# train_df_a2[train_df_a2['target'] == 0].head()\n# train_df_a2[train_df_a2['target'] == 0].loc[train_df_a2['location'].notna()].head()\ntrain_df_a2[\n    (train_df_a2['target'] == 0) &\n    (train_df_a2['location'].str.len() &gt; 1) &\n    (train_df_a2['keyword'].str.len() &gt; 1)\n].sample(frac=1.0).head()\n\n\n\n\nTable 7: Sample of Cleaned Negative Training Data\n\n\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n5997\n8562\nscreams\ngladiator û860û757û\ncasually phone jasmine cries screams spider\n0\n\n\n338\n485\narmageddon\nflightcity uk\nofficial vid gt doublecups gtgt httpstcolfkmtz...\n0\n\n\n5682\n8109\nrescued\nbournemouth\nfinnish hip hop pioneer paleface rescued drift...\n0\n\n\n2598\n3727\ndestroyed\nwaco texas\nalways felt like namekians black people felt p...\n0\n\n\n3510\n5017\neyewitness\nrhode island\nwpri 12 eyewitness news rhode island set moder...\n0\n\n\n\n\n\n\n\n\n\n\nIn the randomly selected tweets there are some differences between the two classes, but after the cleaning the differences are not readily apparent from a content perspective. Both positive and negative tweets have some level of text that is not readily comprehensible.\n\n\n\n3.3.2 Visualizations\n\n\nCode\ndef count_unique_words(input_df, column):\n    # Create a set to store unique words\n    unique_words = set()\n\n    # Iterate through each row in the column\n    for text in input_df[column]:\n        if isinstance(text, str):  # Ensure the entry is a string\n            words = text.split()  # Split into words and normalize to lowercase\n            unique_words.update(words)  # Add words to the set\n\n    # Return the size of the set\n    return len(unique_words)\n\n\n\n\nCode\nresults = [\n    {\n        \"class\": \"original\",\n        \"column\": \"text\",\n        \"count\": count_unique_words(train_df, 'text')\n    },\n    {\n        \"class\": \"a1\",\n        \"column\": \"text\",\n        \"count\": count_unique_words(train_df_a1, 'text')\n    },\n    {\n        \"class\": \"a2\",\n        \"column\": \"text\",\n        \"count\": count_unique_words(train_df_a2, 'text')\n    },\n    {\n        \"class\": \"original\",\n        \"column\": \"keyword\",\n        \"count\": count_unique_words(train_df, 'keyword')\n    },\n    {\n        \"class\": \"a1\",\n        \"column\": \"keyword\",\n        \"count\": count_unique_words(train_df_a1, 'keyword')\n    },\n    {\n        \"class\": \"a2\",\n        \"column\": \"keyword\",\n        \"count\": count_unique_words(train_df_a2, 'keyword')\n    },\n    {\n        \"class\": \"original\",\n        \"column\": \"location\",\n        \"count\": count_unique_words(train_df, 'location')\n    },\n    {\n        \"class\": \"a1\",\n        \"column\": \"location\",\n        \"count\": count_unique_words(train_df_a1, 'location')\n    },\n    {\n        \"class\": \"a2\",\n        \"column\": \"location\",\n        \"count\": count_unique_words(train_df_a2, 'location')\n    },\n]\n\nresults_df = pd.DataFrame(results)\n\nresults_df = results_df.rename({\n\"class\": \"Data Level\",\n\"count\": \"Count\",\n\"column\": \"Data Column\",\n}, axis=\"columns\")\n\ndef plot_count_hist(input_df, column):\n    plt.figure(figsize=(4, 3))\n\n    # Create the barplot\n    ax = sns.barplot(\n        data=input_df.loc[input_df[\"Data Column\"] == column],\n        y=\"Count\",\n        x=\"Data Column\",\n        hue=\"Data Level\",\n    )\n\n    # Customize the legend position to appear below the plot\n    plt.legend(\n        title=\"Data Level\",  # Optional: Add a title to the legend\n        loc=\"upper center\",  # Center the legend horizontally\n        bbox_to_anchor=(0.5, -0.25),  # Adjust the vertical position below the plot\n        ncol=3,  # Display the legend in two columns (optional for compactness)\n        frameon=False,  # Remove the legend border (optional)\n    )\n\n    plt.xlabel(None)\n    plt.tight_layout()  # Adjust layout to prevent overlap\n    plt.show()\n\n\n\n\n\n\n\nCode\nplot_count_hist(results_df, \"text\")\n\n\n\n\n\n\n\n\nFigure 6: Count of unique values in text throughout cleaning process\n\n\n\n\n\n\n\n\n\nCode\nplot_count_hist(results_df, \"location\")\n\n\n\n\n\n\n\n\nFigure 7: Count of unique values in location throughout cleaning process\n\n\n\n\n\n\n\n\n\nCode\nplot_count_hist(results_df, \"keyword\")\n\n\n\n\n\n\n\n\nFigure 8: Count of unique values in keyword throughout cleaning process\n\n\n\n\n\n\n\n\nIn Figure 6 the number of unique values decreases at each data processing step. For the text column removing stop words slightly reduces the unique values count while performing text clean removes a significant number of values. location in Figure 7 follows a similar pattern, but the number of words removed by the stop word cleaning is higher that text. keyword in Figure 8 shows no change to the cleaning suggesting that the data format is defined in the user input and the original data is in a formatted state..\n\n\n\nTable 8: Data Level Descriptions\n\n\n\n\n\n\n\n\n\nData Level\nDescription\n\n\n\n\nOriginal\nOriginal data without modifications\n\n\na1\nStop words removed\n\n\na2\na1 & lowered, removed white space, remove length less than 2, remove punctuation\n\n\n\n\n\n\nWe will now process the training and test data using the above functions and same them to parquet files as input for testing:\n\n\nCode\nfrom pathlib import Path\ndata_path = Path(\"../data/preprocessed\").resolve()\ndata_path.mkdir(exist_ok=True, parents=True)\n\ntrain_data_path = Path(data_path, \"train\")\ntrain_data_path.mkdir(exist_ok=True)\ntrain_raw_filename = Path(train_data_path, \"train_raw.parquet\")\ntrain_a1_filename = Path(train_data_path, \"train_a1.parquet\")\ntrain_a2_filename = Path(train_data_path, \"train_a2.parquet\")\n\ntrain_df.to_parquet(train_raw_filename)\ntrain_df_a1.to_parquet(train_a1_filename)\ntrain_df_a2.to_parquet(train_a2_filename)\n\ntest_data_path = Path(data_path, \"test\")\ntest_data_path.mkdir(exist_ok=True)\ntest_raw_filename = Path(test_data_path, \"test_raw.parquet\")\ntest_a1_filename = Path(test_data_path, \"test_a1.parquet\")\ntest_a2_filename = Path(test_data_path, \"test_a2.parquet\")\n\ntest_df = pd.read_csv(\"../data/test.csv\")\ntest_df_a1 = test_df.copy()\ntest_df_a1 = filter_stop_words(test_df_a1, 'text')\ntest_df_a1 = filter_stop_words(test_df_a1, 'location')\ntest_df_a1 = filter_stop_words(test_df_a1, 'keyword')\n\ntest_df_a2 = test_df_a1.copy()\ntest_df_a2 = clean_df_text_column(test_df_a2, 'text')\ntest_df_a2 = clean_df_text_column(test_df_a2, 'location')\ntest_df_a2 = clean_df_text_column(test_df_a2, 'keyword')\n\ntest_df.to_parquet(test_raw_filename)\ntest_df_a1.to_parquet(test_a1_filename)\ntest_df_a2.to_parquet(test_a2_filename)"
  },
  {
    "objectID": "index.html#tokenization",
    "href": "index.html#tokenization",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.4 Tokenization",
    "text": "3.4 Tokenization\nTo clean the data another step is converting the text into a numerical representation that a neural network can work with. This can be implemented using multiple methods and one popular library is the Transformers Python library developed by Wolf et al. (2022). As detailed by AkaraAsai on GitHub there are many options for pretrained transformers. For this project we will use the bert-base-uncased and bert-base-casedtokenizers via the PreTrainedTokenizer class in the transformers library.\n\n\n\n\nListing 1: Tokenizer Data Loader Implemention\n\n\ndef preprocess_dataframe(\n    df: pd.DataFrame,\n    tokenizer: PreTrainedTokenizer,\n    text_max_length: int,\n    keyword_max_length: int,\n    location_max_length: int,\n):\n    ids, tokens, attentions, targets = [], [], [], []\n\n    max_length = text_max_length + keyword_max_length + location_max_length\n\n    df[\"keyword\"] = df[\"keyword\"].fillna(\"\")\n    df[\"location\"] = df[\"location\"].fillna(\"\")\n\n    for _, row in df.iterrows():\n        # Tokenize each component and add special tokens to indicate type\n        text_tokens = tokenizer.encode(\n            row[\"text\"],\n            add_special_tokens=True,\n            truncation=True,\n            max_length=text_max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        ).tolist()[0]\n        keyword_tokens = tokenizer.encode(\n            row[\"keyword\"],\n            add_special_tokens=True,\n            truncation=True,\n            max_length=keyword_max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        ).tolist()[0]\n        location_tokens = tokenizer.encode(\n            row[\"location\"],\n            add_special_tokens=True,\n            truncation=True,\n            max_length=location_max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        ).tolist()[0]\n\n        # Combine tokens\n        combined_tokens = text_tokens + keyword_tokens + location_tokens\n\n        # Create initial attention mask with 1s for all tokens\n        attention_mask = [1] * len(combined_tokens)\n\n        # Pad tokens and attention mask to max_length\n        padding_length = max_length - len(combined_tokens)\n        combined_tokens += [tokenizer.pad_token_id] * padding_length\n        attention_mask += [0] * padding_length\n\n        # Update attention mask to set positions with padding tokens to 0\n        attention_mask = [\n            0 if token == tokenizer.pad_token_id else mask\n            for token, mask in zip(combined_tokens, attention_mask)\n        ]\n\n        # Collect processed data\n        ids.append(row[\"id\"])\n        tokens.append(combined_tokens)\n        attentions.append(attention_mask)\n        try:\n            targets.append(row[\"target\"])\n        except KeyError:\n            continue\n\n    # Return targets if they exist\n    if len(targets) &gt; 0:\n        return pd.DataFrame(\n            {\"id\": ids, \"tokens\": tokens, \"attention\": attentions, \"target\": targets}\n        )\n    else:\n        return pd.DataFrame({\"id\": ids, \"tokens\": tokens, \"attention\": attentions})"
  },
  {
    "objectID": "index.html#embedding-layers",
    "href": "index.html#embedding-layers",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.1 Embedding Layers",
    "text": "4.1 Embedding Layers\nThe model uses three separate embedding layers (text_embedding, keyword_embedding, and location_embedding) to convert the categorical input (text, keyword, and location) into dense, low-dimensional vectors.\n\nEmbedding Dimension: The size of these dense representations is a tunable hyperparameter (embedding_dim, default: 128).\nPadding Index: A padding index of 0 ensures uniform sequence lengths for inputs with varying sizes."
  },
  {
    "objectID": "index.html#recurrent-layer-lstm",
    "href": "index.html#recurrent-layer-lstm",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.2 Recurrent Layer (LSTM)",
    "text": "4.2 Recurrent Layer (LSTM)\nThe embeddings are concatenated into a combined representation, which serves as input to an LSTM (Long Short-Term Memory) layer first described by Hochreiter and Schmidhuber (1997). For this project the input dimension is can be tuned as a hyperparameter:\n\nInput Dimensions\n\nThe concatenated embeddings have a dimensionality of \\text{Embedding Dimension} \\times 3 as all three embeddings are combined.\n\nHidden Dimension The LSTM layer processes the input and outputs a hidden state with size hidden_dim. To reduce the number of hyperparamaters this value is fixed at \\text{Input Dimension} * 2\nBatch Processing: The batch_first=True argument ensures the input tensors are structured as (batch size, sequence length, feature size)."
  },
  {
    "objectID": "index.html#fully-connected-layer",
    "href": "index.html#fully-connected-layer",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.3 Fully Connected Layer",
    "text": "4.3 Fully Connected Layer\nThe final hidden state of the LSTM, corresponding to the last time step, is passed through a fully connected (fc) layer to reduce the dimensionality to the output size of 1."
  },
  {
    "objectID": "index.html#sigmoid-activation",
    "href": "index.html#sigmoid-activation",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.4 Sigmoid Activation",
    "text": "4.4 Sigmoid Activation\nThe output of the fully connected layer is passed through a sigmoid activation function, which scales the predictions to a range of [0, 1]. These outputs represent the probability of a tweet being disaster-related. For validation or testing, these probabilities are thresholded (e.g., \\geq 0.5) to classify the output as either 1 or 0."
  },
  {
    "objectID": "index.html#rnnwithmultiinput-class-definition",
    "href": "index.html#rnnwithmultiinput-class-definition",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.5 RNNWithMultiInput Class Definition",
    "text": "4.5 RNNWithMultiInput Class Definition\n\n\n\n\nListing 2: RNN Pytorch Model\n\n\nimport torch\nimport torch.nn as nn\n\nclass RNNWithMultiInput(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        use_attention=False,\n        embedding_dim=128,\n        hidden_dim=256,\n        output_dim=1,\n    ):\n        super(RNNWithMultiInput, self).__init__()\n        self.use_attention = use_attention\n        self.text_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.keyword_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.location_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n\n        self.lstm = nn.LSTM(\n            embedding_dim * 3,\n            hidden_dim,\n            batch_first=True,\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(\n        self,\n        text_input_ids,\n        text_attention_mask,\n        keyword_input_ids,\n        keyword_attention_mask,\n        location_input_ids,\n        location_attention_mask,\n    ):\n        # Embedding layers\n        text_emb = self.text_embedding(text_input_ids)\n        keyword_emb = self.keyword_embedding(keyword_input_ids)\n        location_emb = self.location_embedding(location_input_ids)\n\n        if self.use_attention is True:\n            text_emb = text_emb * text_attention_mask.unsqueeze(-1)\n            keyword_emb = keyword_emb * keyword_attention_mask.unsqueeze(-1)\n            location_emb = location_emb * location_attention_mask.unsqueeze(-1)\n\n        # Combine embeddings\n        combined_emb = torch.cat((text_emb, keyword_emb, location_emb), dim=2)\n\n        # Pass through LSTM\n        lstm_out, _ = self.lstm(combined_emb)\n        last_hidden_state = lstm_out[:, -1, :]\n\n        # Fully connected layer\n        logits = self.fc(last_hidden_state)\n        return self.sigmoid(logits).squeeze()"
  },
  {
    "objectID": "index.html#hyperparameter-tuning",
    "href": "index.html#hyperparameter-tuning",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.6 Hyperparameter Tuning",
    "text": "4.6 Hyperparameter Tuning\nKey hyperparameters for optimization include:\n\nEmbedding Dimension (embedding_dim): Controls the size of the feature space for text representation.\nHidden Dimension (hidden_dim): Determines the capacity of the LSTM layer to capture sequential patterns.\nBatch Size and Learning Rate: While not part of the architecture, these parameters significantly influence training efficiency and model performance.\n\n\n4.6.1 Attention Mechanism Hyperparameter\nThe model includes an optional attention mechanism (enabled by the use_attention flag). If enabled, attention masks are applied to the embeddings to emphasize relevant parts of the input sequences while ignoring padded elements."
  },
  {
    "objectID": "index.html#rationale-for-architecture",
    "href": "index.html#rationale-for-architecture",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.7 Rationale for Architecture",
    "text": "4.7 Rationale for Architecture\nThis architecture is well-suited for this problem because:\n\nThe LSTM layer efficiently captures sequential dependencies in textual data, which is critical for understanding the context within tweets.\nBy incorporating separate embeddings for keyword and location, the model leverages additional information beyond the tweet text, potentially improving classification accuracy.\nThe flexibility to enable or disable attention mechanisms provides adaptability for datasets with varying levels of noise or irrelevant data."
  },
  {
    "objectID": "index.html#baseline-models",
    "href": "index.html#baseline-models",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.1 Baseline Models",
    "text": "5.1 Baseline Models\n\n5.1.1 Training Loss and F1 Score\n\n\nCode\ndf_baseline = df.loc[\n    (df[\"Comparison Type\"] == 'Baseline')\n]\n\n\n\n\nCode\nplot_vs_embedding_dim(\n    df_baseline,\n    \"Training F1 Score\",\n    show_legend=False,\n    ylim=1.0\n)\nplot_vs_embedding_dim(\n    df_baseline,\n    \"Validation F1 Score\",\n    ylim=1.0,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) Validation F1 Score\n\n\n\n\n\n\n\nFigure 9: Baseline Model - Training and Validation F1 Score\n\n\n\n\n5.1.2 Learning Rate and Compute Time\n\nCode\nplot_vs_embedding_dim(\n    df_baseline,\n    \"Learning Rate\",\n    show_legend=False,\n)\nplot_vs_embedding_dim(\n    df_baseline,\n    \"Compute Time\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Learning Rate\n\n\n\n\n\n\n\n\n\n\n\n(b) Compute Time Per Epoch [s]\n\n\n\n\n\n\n\nFigure 10: Baseline Model - Learning Rate and Compute Time\n\n\n\n\n\nFigure Figure 9 (a) illustrates that across embedding dimensions, the training F1 score consistently improves with an increasing number of epochs. Models with lower embedding dimensions, however, converge to a lower final training F1 score, suggesting that these dimensions may limit the model’s complexity capacity. Similarly, in Figure Figure 9 (b), validation F1 scores also increase with training epochs. However, an upper limit is observed, where embedding dimensions above 16 do not demonstrate a significant difference in validation performance.\nBased on these trends, training for 50 epochs appears sufficient for models with embedding dimensions greater than 8 to achieve training stability and reach their performance plateau.\nFigure Figure 10 (a) visualizes the learning rate progression over training epochs. The learning rate is adjusted dynamically using a ReduceLROnPlateau scheduler based on validation F1 scores, as shown in the code snippet below:\n\n\n\n\nListing 3: Learning Rate Scheduler\n\n\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode=\"max\",\n)\n# ...\nscheduler.step(val_f1)\n\n\n\n\nModels with higher embedding dimensions tend to trigger the scheduler earlier, indicating that these models converge more rapidly. While this is advantageous for reducing the risk of overfitting, it does not necessarily translate into improved validation performance beyond the observed upper limit.\nFigure 10 (b) examines the compute times per epoch. Models with embedding dimensions below 256 maintain consistent training times of approximately 2 seconds per epoch. However, as embedding dimensions increase, so do computation times. Given that larger models fail to deliver meaningful performance improvements, it is computationally efficient to select a smaller model that balances performance and training cost effectively."
  },
  {
    "objectID": "index.html#tokenizer-comparison",
    "href": "index.html#tokenizer-comparison",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.2 Tokenizer Comparison",
    "text": "5.2 Tokenizer Comparison\n\n\nCode\ndf_bert_uncased = df.loc[\n    (df['Comparison Type'] == 'Tokenizer') & (df['Tokenizer'] == 'bert-base-uncased')\n]\n\ndf_bert_cased = df.loc[\n    (df['Comparison Type'] == 'Tokenizer') & (df['Tokenizer'] == 'bert-base-cased')\n]\n\n\n\n\nCode\nplot_vs_embedding_dim(df_bert_uncased, \"Training F1 Score\", ylim=1.0)\nplot_vs_embedding_dim(df_bert_cased, \"Training F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Uncased Tokenizer Training Loss\n\n\n\n\n\n\n\n\n\n\n\n(b) Cased Tokenizer Training Loss\n\n\n\n\n\n\n\nFigure 11: Uncased vs Cased Tokenizer - Training Loss\n\n\n\n\nCode\nplot_vs_embedding_dim(df_bert_uncased, \"Validation F1 Score\", ylim=1.0)\nplot_vs_embedding_dim(df_bert_cased, \"Validation F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Uncased Tokenizer F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) Cased Tokenizer F1 Score\n\n\n\n\n\n\n\nFigure 12: Uncased vs Cased Tokenizer - F1 Score\n\n\n\n\nFigure 11 and Figure 12 show no significant differences in performance between the cased and uncased tokenizers, as measured by training and validation F1 scores. However, since the original dataset retains case sensitivity, preserving this feature through a cased tokenizer aligns with the original characteristics of the data. This decision ensures that potentially meaningful information encoded in capitalization is retained."
  },
  {
    "objectID": "index.html#data-level-comparison",
    "href": "index.html#data-level-comparison",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.3 Data Level Comparison",
    "text": "5.3 Data Level Comparison\n\n\nCode\ndf_data_original = df.loc[\n    (df['Comparison Type'] == 'Data Level') & (df['Data Level'] == 'original')\n]\n\ndf_data_a1 = df.loc[\n    (df['Comparison Type'] == 'Data Level') & (df['Data Level'] == 'a1')\n]\n\ndf_data_a2 = df.loc[\n    (df['Comparison Type'] == 'Data Level') & (df['Data Level'] == 'a2')\n]\n\n\n\n\nCode\nplot_vs_embedding_dim(df_data_original, \"Training F1 Score\", ylim=1.0)\nplot_vs_embedding_dim(df_data_a1, \"Training F1 Score\", ylim=1.0)\nplot_vs_embedding_dim(df_data_a2, \"Training F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Data - Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) A1 Data - Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(c) A2 Data - Training F1 Score\n\n\n\n\n\n\n\nFigure 13: Training F1 Score by Data Levels\n\n\n\n\nCode\nplot_vs_embedding_dim(df_data_original, \"Validation F1 Score\", ylim=1.0)\nplot_vs_embedding_dim(df_data_a1, \"Validation F1 Score\", ylim=1.0)\nplot_vs_embedding_dim(df_data_a2, \"Validation F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Data - Validation F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) A1 Data - Validation F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(c) A2 Data - Validation F1 Score\n\n\n\n\n\n\n\nFigure 14: Validation F1 Score by Data Levels\n\n\n\n\nFigure 13 and Figure 14 show no difference in F1 score between diferent data levels. Because altering the data level changes the way the tokenizer functions we will keep the original (unaltered) data as input into the tokenizer\nFigures Figure 13 and Figure 14 indicate no measurable difference in F1 scores across various data preprocessing levels. Given that altering the data level modifies how the tokenizer processes the input, it introduces additional complexity without yielding performance benefits. Based on this finding we will use the original unaltered data as input to subsequent models."
  },
  {
    "objectID": "index.html#embedding-layers-1",
    "href": "index.html#embedding-layers-1",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.4 1-3 Embedding Layers",
    "text": "5.4 1-3 Embedding Layers\n\n\nCode\ndf_data_one_layer = df.loc[\n    (df['Comparison Type'] == 'Layers') & (df['Embedding Layers'] == 1)\n]\n\ndf_data_two_layer = df.loc[\n    (df['Comparison Type'] == 'Layers') & (df['Embedding Layers'] == 2)\n]\n\ndf_data_three_layer = df.loc[\n    (df['Comparison Type'] == 'Layers') & (df['Embedding Layers'] == 3)\n]\n\n\n\n\nCode\nplot_vs_embedding_dim(df_data_one_layer, \"Training F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_two_layer, \"Training F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_three_layer, \"Training F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 1 Embedding Layer - Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) 2 Embedding Layers - Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(c) 3 Embedding Layers - Training F1 Score\n\n\n\n\n\n\n\nFigure 15: Training F1 Score by Data Levels\n\n\n\n\nCode\nplot_vs_embedding_dim(df_data_one_layer, \"Validation F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_two_layer, \"Validation F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_three_layer, \"Validation F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 1 Embedding Layer - Validation F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) 2 Embedding Layers - Validation F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(c) 3 Embedding Layers - Validation F1 Score\n\n\n\n\n\n\n\nFigure 16: Validation F1 Score by Data Levels"
  },
  {
    "objectID": "index.html#embedding-layers-2",
    "href": "index.html#embedding-layers-2",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.5 4-6 Embedding Layers",
    "text": "5.5 4-6 Embedding Layers\n\n\nCode\ndf_data_one_layer = df.loc[\n    (df['Comparison Type'] == 'Big Layers') & (df['Embedding Layers'] == 4)\n]\n\ndf_data_two_layer = df.loc[\n    (df['Comparison Type'] == 'Big Layers') & (df['Embedding Layers'] == 5)\n]\n\ndf_data_three_layer = df.loc[\n    (df['Comparison Type'] == 'Big Layers') & (df['Embedding Layers'] == 6)\n]\n\n\n\n\nCode\nplot_vs_embedding_dim(df_data_one_layer, \"Training F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_two_layer, \"Training F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_three_layer, \"Training F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 4 Embedding Layers - Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) 5 Embedding Layers - Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(c) 6 Embedding Layers - Training F1 Score\n\n\n\n\n\n\n\nFigure 17: Training F1 Score by Data Levels\n\n\n\n\nCode\nplot_vs_embedding_dim(df_data_one_layer, \"Validation F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_two_layer, \"Validation F1 Score\", show_legend=False, ylim=1.0)\nplot_vs_embedding_dim(df_data_three_layer, \"Validation F1 Score\", ylim=1.0)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 4 Embedding Layers - Validation F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) 5 Embedding Layers - Validation F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(c) 6 Embedding Layers - Validation F1 Score\n\n\n\n\n\n\n\nFigure 18: Validation F1 Score by Data Levels\n\n\n\nIncreasing the embedding dimensions has a subtle but observable effect on the validation F1 score. Models with higher embedding dimensions inherently introduce dropout, which helps mitigate overfitting by regularizing the network. This regularization contributes to greater training stability, as evidenced by reduced fluctuations in F1 scores across epochs. While the performance gains are marginal, the enhanced stability offered by larger embedding dimensions may provide a slight advantage for tasks requiring consistent and reliable predictions."
  },
  {
    "objectID": "index.html#final-models",
    "href": "index.html#final-models",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.6 Final Models",
    "text": "5.6 Final Models\n\n\nCode\ndf_final = df.loc[\n    df['Comparison Type'] == 'Final'\n]\n\n\n\n\nCode\nplot_vs_embedding_dim(df_final, \"Training F1 Score\", hue = \"Embedding Layers\", show_legend=False)\nplot_vs_embedding_dim(df_final, \"Validation F1 Score\", hue = \"Embedding Layers\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Training F1 Score\n\n\n\n\n\n\n\n\n\n\n\n(b) Validation F1 Score\n\n\n\n\n\n\n\nFigure 19: Final Model F1 Scores\n\n\n\n\nIn the final output models, we observe a steady increase in the training F1 score across all models, while the validation F1 score stabilizes after ~30 epochs. Notably, models with 2 and 6 embedding layers achieve higher validation F1 scores compared to those with 4 embedding layers. This suggests that embedding layer depth plays a nuanced role in model performance, with certain configurations better capturing the underlying patterns in the data. All models demonstrate the ability to fit the input data effectively and achieve stability within the specified 50 epochs."
  },
  {
    "objectID": "index.html#kaggle-screenshot",
    "href": "index.html#kaggle-screenshot",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.1 Kaggle Screenshot",
    "text": "6.1 Kaggle Screenshot\n\n\n\n\n\n\nFigure 20: Kaggle Results"
  },
  {
    "objectID": "index.html#results-analysis",
    "href": "index.html#results-analysis",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.2 Results Analysis",
    "text": "6.2 Results Analysis\n\n\nCode\ndf_final = df_final.loc[df_final['Epoch'] == 50]\npublic_scores = [0.75973, 0.73490, 0.73337]\ndf_final[\"Public Score\"] = public_scores\n\n\n\n\nCode\n# Reshape the data for a grouped bar plot\ndf_melted = df_final.melt(\n    id_vars=\"Embedding Layers\",\n    value_vars=[\"Public Score\"],\n    var_name=\"Score Type\",\n    value_name=\"Score\",\n)\n\n# Create the grouped bar plot\nplt.figure(figsize=(10, 3))\nax = sns.barplot(data=df_melted, x=\"Embedding Layers\", y=\"Score\", hue=\"Score Type\")\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")\nplt.ylim((0, 0.85))\nplt.title(\"Scores by Embedding Layers\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 21: Final Model Scores\n\n\n\n\n\n\n\nCode\npd.set_option('display.max_rows', 500)\ndf_melted\n\n\n\n\nTable 9: Final Model Scores\n\n\n\n\n\n\n\n\n\n\nEmbedding Layers\nScore Type\nScore\n\n\n\n\n0\n2\nPublic Score\n0.75973\n\n\n1\n4\nPublic Score\n0.73490\n\n\n2\n6\nPublic Score\n0.73337\n\n\n\n\n\n\n\n\n\n\nIn Figure 21 and Table 9 final model Kaggle scores are visualized and tabulated. For each model the other specifications are:\nIn Figure 21 and Table 9, the final Kaggle scores for each model are visualized and tabulated. These scores represent the performance of the models on the public leaderboard, providing a benchmark for comparison. Based on these results, the model with 2 embedding layers achieved the highest public score, suggesting that simpler architectures may offer better generalization for this task. This outcome aligns with observations from the validation F1 scores, where models with fewer embedding layers demonstrated comparable or superior performance relative to more complex configurations.\n\n6.2.1 Final Model Specifications\n\n\nCode\nfrom IPython.display import display, Markdown\n\ndf_final['Start Learning Rate'] = 0.001\ndf_final['End Learning Rate'] = df['Learning Rate']\n\ndf_specs = (\n    df_final[\n        [\n            \"Learning Optimization\",\n            \"Start Learning Rate\",\n            \"End Learning Rate\",\n            \"Specified Epochs\",\n            \"Batch Size\",\n            \"Data Level\",\n            \"Vocab Size\",\n            \"Tokenizer\",\n            \"Embedding Dimensions\",\n            \"Hidden Dimensions\",\n        ]\n    ]\n    .iloc[0]\n    .T\n)\n\n# Convert series into df\ndf_specs = df_specs.reset_index()\ndf_specs.columns = [\"Specification\", \"Value\"]\n\ndisplay(Markdown(df_specs.to_markdown(index=False)))\n\n\n\n\nTable 10: Final Model Specifications\n\n\n\n\n\n\nSpecification\nValue\n\n\n\n\nLearning Optimization\ndefault\n\n\nStart Learning Rate\n0.001\n\n\nEnd Learning Rate\n1.0000000000000002e-06\n\n\nSpecified Epochs\n50\n\n\nBatch Size\n256\n\n\nData Level\noriginal\n\n\nVocab Size\n28996\n\n\nTokenizer\nbert-base-cased\n\n\nEmbedding Dimensions\n128\n\n\nHidden Dimensions\n256\n\n\n\n\n\n\n\n\n\n\n6.2.2 Additional Final Model Statistics\n\n\nCode\n# Reshape the data for a grouped bar plot\ndf_melted = df_final.melt(\n    id_vars=\"Embedding Layers\",\n    value_vars=[\"Validation Accuracy Score\", \"Validation Precision Score\", \"Validation Recall Score\"],\n    var_name=\"Score Type\",\n    value_name=\"Score\",\n)\n\n# Create the grouped bar plot\nplt.figure(figsize=(10, 3))\nax = sns.barplot(data=df_melted, x=\"Embedding Layers\", y=\"Score\", hue=\"Score Type\")\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")\nplt.ylim((0, 0.85))\nplt.title(\"Scores by Embedding Layers\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 22: Final Model Scores - Kaggle Public Score vs. Accuracy Score\n\n\n\n\n\n\n\nCode\npd.set_option('display.max_rows', 500)\ndf_melted\n\n\n\n\nTable 11: Final Model Additional Statistics\n\n\n\n\n\n\n\n\n\n\nEmbedding Layers\nScore Type\nScore\n\n\n\n\n0\n2\nValidation Accuracy Score\n0.768221\n\n\n1\n4\nValidation Accuracy Score\n0.741300\n\n\n2\n6\nValidation Accuracy Score\n0.749836\n\n\n3\n2\nValidation Precision Score\n0.781197\n\n\n4\n4\nValidation Precision Score\n0.737624\n\n\n5\n6\nValidation Precision Score\n0.730475\n\n\n6\n2\nValidation Recall Score\n0.670088\n\n\n7\n4\nValidation Recall Score\n0.655425\n\n\n8\n6\nValidation Recall Score\n0.699413\n\n\n\n\n\n\n\n\n\n\nIn addition to the public Kaggle scores, the models were validated using accuracy, precision, and recall, and F1 score, shown in Figure 21 and Table 11. These metrics provide a comprehensive view of model performance and highlight differences in how the models handle the classification task.\n\nAccuracy:\n\nThe model with 2 embedding layers achieved the highest validation accuracy (0.768221), outperforming both the 4-layer and 6-layer models.\nWhile the accuracy decreases slightly with an increase in embedding layers, the difference is small.\n\nPrecision:\n\nPrecision is highest for the 2-layer model (0.781197), indicating its strength in minimizing false positives.\nAs the number of embedding layers increases, precision declines, with the 6-layer model scoring the lowest (0.730475).\n\nRecall:\n\nRecall improves with more embedding layers, with the 6-layer model achieving the highest score (0.699413). This suggests that models with more embedding layers are better at capturing true positives, albeit at the expense of increased false positives."
  },
  {
    "objectID": "index.html#project-summary",
    "href": "index.html#project-summary",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.1 Project Summary",
    "text": "7.1 Project Summary\nThis project explored the classification of disaster-related tweets using Recurrent Neural Networks (RNNs) built with PyTorch. Various configurations, including embedding dimensions, tokenizer choices, and data cleaning levels, were systematically evaluated to tune hyperparameters optimally. The results demonstrated that a 2-layer embedding model achieved the highest overall performance, balancing validation accuracy (0.768221), precision (0.781197), recall (0.670088), and F1 score (0.721389) and a public Kaggle evaluation scores of 0.75973. The findings underscore the value of simplicity in model architecture, with more complex configurations yielding diminishing returns."
  },
  {
    "objectID": "index.html#lessons-learned",
    "href": "index.html#lessons-learned",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.2 Lessons Learned",
    "text": "7.2 Lessons Learned\n\nModel Architecture and Complexity: Increasing embedding layers and introducing dropout led to marginal stability improvements but did not significantly enhance validation F1 scores. Simpler architectures performed comparably or better in most cases.\nTokenizer and Data Cleaning: The cased tokenizer demonstrated equivalent performance to the uncased version, justifying the retention of the original data’s case sensitivity. Altering data levels disrupted tokenizer behavior without providing measurable benefits.\nLayer Size: Models with fewer embedding layers converged faster and triggered the learning rate scheduler earlier, suggesting efficiency advantages in training. All models reached stability within 50 epochs, highlighting the importance of early stopping to reduce computational overhead.\nEvaluation Metrics: The F1 score proved to be the most informative metric for this dataset, balancing precision and recall effectively. Relying solely on accuracy or public Kaggle scores would have overlooked critical trade-offs in model performance."
  },
  {
    "objectID": "index.html#areas-for-improvement-future-work",
    "href": "index.html#areas-for-improvement-future-work",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.3 Areas for Improvement / Future Work",
    "text": "7.3 Areas for Improvement / Future Work\n\nFeature Engineering: Incorporating additional features, such as sentiment analysis scores or tweet metadata, could enhance the models ability to capture nuanced patterns in the data.\nArchitectural Changes: Future experiments could include transformer-based architectures, such as BERT or GPT, to assess whether advanced models outperform RNNs for this task.\nGeneralization Analysis: While this project focused on disaster-related tweets, extending the dataset to include non-disaster events could help test the model’s generalization capabilities across broader text classification domains."
  }
]